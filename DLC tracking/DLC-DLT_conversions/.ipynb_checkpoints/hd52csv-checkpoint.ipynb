{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce4cc96",
   "metadata": {},
   "source": [
    "Code edited from https://github.com/backyardbiomech/DLCconverterDLT/tree/master \\\n",
    "Tanvi Deora \\\n",
    "May 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b54cd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deeplabcut'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeeplabcut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauxiliaryfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_config\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deeplabcut'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import re\n",
    "from deeplabcut.utils.auxiliaryfunctions import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef99942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlc2dlt(camera_h5_paths, config_path, outpath, key, like):\n",
    "    # Converts DeepLabCut .h5 files back to .csv for DLTdv software\n",
    "    # these assumes single individual and does not account for offsets\n",
    "    \n",
    "    # INPUTS:\n",
    "    # 1) camera_h5_paths = list(cam1_h5_paths, cam2_h5_path, cam3_h5_path) --> please specify the .h5 files cameras with in the correct order - [cam1, cam2 and cam3] etc\n",
    "    # 2) config --> path to the config file\n",
    "    # 3) key --> number to string mapping for different body parts\n",
    "    # 4) outpath --> output path\n",
    "    \n",
    "    config=Path(config_path)\n",
    "    opath = Path(outpath)\n",
    "\n",
    "    numcams=len(camera_h5_paths)\n",
    "    like = 0.9\n",
    "\n",
    "    # load dlc config\n",
    "    cfg = read_config(config)\n",
    "#     scorer = cfg['scorer']\n",
    "    bodyparts=cfg['bodyparts']\n",
    "    coords = ['x', 'y']\n",
    "    tracks=bodyparts\n",
    "\n",
    "    # alldata is a nested dict to contain numpy arrays until writing to csv\n",
    "    # first key is cam\n",
    "    alldata = {}\n",
    "    numframes = []\n",
    "\n",
    "    # load each data file get some basic info and store data in a dict\n",
    "\n",
    "    for c in range(numcams):\n",
    "            #load the hd5\n",
    "            camdata = pd.read_hdf(camera_h5_paths[c])\n",
    "            scorer = camdata.columns.get_level_values(0).unique()[0]\n",
    "\n",
    "            for track in set(tracks):\n",
    "                    camdata.loc[camdata[scorer][track]['likelihood'] <= like, (scorer, track, ['x', 'y'])] = np.nan\n",
    "                    alldata[c] = camdata[scorer]\n",
    "\n",
    "            # make a list to keep track of the number of frames in each camera's dataset\n",
    "#             frames = camdata.index.get_level_values(2)\n",
    "#             no_frames = [s[3:-4] for s in frames]\n",
    "            numframes.append(max(camdata.index) + 1)\n",
    "\n",
    "    # initialize the massive array full of nans (with more than enough rows)\n",
    "    blankarr = np.empty((max(numframes), len(tracks) * 2 * numcams)) * np.nan\n",
    "    xcols = range(0, len(tracks) * 2 * numcams, 2)\n",
    "    ycols = range(1, len(tracks) * 2 * numcams, 2)\n",
    "    outdata = {}\n",
    "    outdata[0] = blankarr.copy()\n",
    "\n",
    "    #loop through each camera's data and assign to the proper row\n",
    "\n",
    "    for c, camdata in alldata.items():\n",
    "        # make lists of rows, of matching length, that account for offsets (out = in - offset)\n",
    "        outrows = list(range(0, numframes[c], 1))\n",
    "        inrows = list(range(0, numframes[c], 1))\n",
    "        for i in range(len(tracks)):\n",
    "            incol = i\n",
    "            outcol = (i * 2 * numcams) + (2 * c)\n",
    "            outdata[0][:, outcol:outcol+2] = camdata.iloc[:, incol:incol+2]\n",
    "\n",
    "    #                 if flipy:\n",
    "    #                     outdata[0][outrows, outcol] = camdata.iloc[inrows, incol]\n",
    "    #                     outdata[0][outrows, outcol+1] = height - camdata.iloc[inrows, incol+1]\n",
    "    #                 else:\n",
    "\n",
    "    # make col names (same for all files)\n",
    "\n",
    "    # map names of points in DLC to pt1, pt2 etc in DLT\n",
    "\n",
    "    inv_map = {v: k for k, v in key.items()}\n",
    "    new_tracks = [inv_map[t] for t in tracks]\n",
    "    xycols = ['{}_cam_{}_{}'.format(x, c, d) for x in new_tracks for c in range(1,numcams+1) for d in ['x', 'y']]\n",
    "\n",
    "    ind=0\n",
    "    basename = str(opath) + '-'\n",
    "    xydf = pd.DataFrame(outdata[ind], columns=xycols, index=range(len(outdata[ind])))\n",
    "    xydf.to_csv((basename + 'xypts.csv'), na_rep=\"NaN\", index=False)\n",
    "\n",
    "    # make \"dummy\" files\n",
    "    xyzcols = ['{}_{}'.format(x, d) for x in new_tracks for d in ['x', 'y', 'z']]\n",
    "    xyzdf = pd.DataFrame(np.nan, columns=xyzcols, index=range(len(outdata[ind])))\n",
    "    xyzdf.to_csv((basename + 'xyzpts.csv'), na_rep='NaN', index=False)\n",
    "\n",
    "    residdf = pd.DataFrame(np.nan, columns=new_tracks, index=range(len(outdata[ind])))\n",
    "    residdf.to_csv((basename + 'xyzres.csv'), na_rep='NaN', index=False)\n",
    "\n",
    "    offcols = ['cam_{}'.format(cnum) for cnum in range(1, numcams + 1)]\n",
    "    offdf = pd.DataFrame(0, columns=offcols, index=range(len(outdata[ind])))\n",
    "    # offdf.iloc[0] = offsets\n",
    "    offdf.to_csv((basename + 'offsets.csv'), na_rep='NaN', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483fe5f",
   "metadata": {},
   "source": [
    "### Analyze individual videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41a5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam1_h5_path = './reextmeetingthisweeknishkaspresentation/Trial2_400rpm_16024DLC_Resnet50_Chase_SideViewMar25shuffle1_snapshot_060.h5'\n",
    "cam2_h5_path = './reextmeetingthisweeknishkaspresentation/Trial2_400rpm_20655DLC_Resnet50_Chase_SideViewMar25shuffle1_snapshot_060.h5'\n",
    "cam3_h5_path = './reextmeetingthisweeknishkaspresentation/Trial2_400rpm_20654DLC_Resnet50_MuscaChase_TopViewMar24shuffle1_snapshot_010.h5'\n",
    "\n",
    "h5_path = [cam1_h5_path, cam2_h5_path, cam3_h5_path]\n",
    "\n",
    "config_path = './reextmeetingthisweeknishkaspresentation/config.yaml'\n",
    "\n",
    "# outpath is path + name of video\n",
    "vidname = 'Trial2_400rpm'\n",
    "outpath = './reextmeetingthisweeknishkaspresentation/' + vidname\n",
    "\n",
    "key = {'pt1':'bead',\n",
    "       'pt2': 'head',\n",
    "        'pt3':'left_wing_base',\n",
    "       'pt4': 'right_wing_base',\n",
    "       'pt5': 'left_wing_tip',\n",
    "       'pt6': 'right_wing_tip',\n",
    "       'pt7': 'abdomen_tip',\n",
    "       'pt8': 'left_hindL_tip',\n",
    "       'pt9': 'right_hindL_tip',\n",
    "       'pt10': 'left_midL_tip',\n",
    "       'pt11': 'right_midL_tip',\n",
    "       'pt12': 'left_foreL_tip',\n",
    "       'pt13': 'right_foreL_tip'\n",
    "      }\n",
    "\n",
    "like = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc2dlt(h5_path, config_path, outpath, key, like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4d2ff",
   "metadata": {},
   "source": [
    "### batch process all data\n",
    "#### adapted from Girish's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5b8c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all input h5 files\n",
    "all_cam1_paths=glob.glob(r\"./reextmeetingthisweeknishkaspresentation/cam1/*.h5\")\n",
    "all_cam2_paths=glob.glob(r\"./reextmeetingthisweeknishkaspresentation/cam2/*.h5\")\n",
    "all_cam3_paths=glob.glob(r\"./reextmeetingthisweeknishkaspresentation/cam3/*.h5\")\n",
    "list_of_camlist = list(zip(all_cam1_paths, all_cam2_paths, all_cam3_paths))\n",
    "\n",
    "# make a list of output paths with correct filenames\n",
    "basenames = []\n",
    "for camlist in list_of_camlist:\n",
    "    filenames = []\n",
    "    for c, cam_name in enumerate(camlist):\n",
    "        filenames.append('_'.join(cam_name[0:cam_name.find(\"DLC_\")].split('\\\\')[-1].split('_')[0:2]))\n",
    "    all_same = len(set(filenames)) == 1\n",
    "    if all_same:\n",
    "        bb = filename\n",
    "    else:\n",
    "        print(f\"Check input files for {cam_list} ,filenames are different\")\n",
    "    basenames.append(bb)\n",
    "\n",
    "outpath = './reextmeetingthisweeknishkaspresentation/'\n",
    "outpath_list = [outpath + name for name in basenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './reextmeetingthisweeknishkaspresentation/config.yaml' \n",
    "\n",
    "key = {'pt1':'bead',\n",
    "       'pt2': 'head',\n",
    "        'pt3':'left_wing_base',\n",
    "       'pt4': 'right_wing_base',\n",
    "       'pt5': 'left_wing_tip',\n",
    "       'pt6': 'right_wing_tip',\n",
    "       'pt7': 'abdomen_tip',\n",
    "       'pt8': 'left_hindL_tip',\n",
    "       'pt9': 'right_hindL_tip',\n",
    "       'pt10': 'left_midL_tip',\n",
    "       'pt11': 'right_midL_tip',\n",
    "       'pt12': 'left_foreL_tip',\n",
    "       'pt13': 'right_foreL_tip'\n",
    "      }\n",
    "\n",
    "like = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa165b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_list, outpath in zip(list_of_camlist, outpath_list):\n",
    "    dlc2dlt(input_list, config_path, outpath, key, like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33491978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
